[2023-11-03 10:55]

Plan: 
- Explore data: plot some images, understand format, get basic dataset statistics

- Train U-Net from scratch
    - output channels: 1 + thresholding, or 2 + softmax
    - MSE
    - eval MSE, maybe structural metrics
- Cookiecutter template: start with notebooks, maybe turn into py modules later

Doing:
- Download dataset
- Exploratory Data Analysis notebook - read data, preprocess, plot samples

[2023-11-05 23:55]
Tiling done
- crop, pad, mask
- untiling needs testing #DONE#

[2023-11-07 14:02]
Training works

Plan:
- Eval notebook - visualize predictions
- ONNX runner

- dataset handling
    - fixed dataset splits filelists, contain only filenames
    - all preprocessing done live
    - directory structure assumed:
        <dataset_root>
            - filelists
                - train/val/test.csv
            - scenes
                - <file1..N>
            - masks
                - <file1..N>


- Model output and postprocessing
    - output logits, BCEWithLogitsLoss, manual sigmoid() after, or use .predict()
    - 2 channels + softmax within the model

[2023-11-08 00:37]
Next:
- Train on full dataset  #DONE#
- Log to W&B  #DONE#
- Train padded
- Use padding and mask
- 7mins+ for datasets loading  #DONE#
- Augmentations?

metacentrum packages missing:
torchinfo
wandb  # praha
segmentation_models_pytorch

Next:
- mirror back changes from metacentrum  #DONE#
- Own eval metrics impl  #DONE#
- Wandb table sample predictions  #DONE#
- Dice loss  #DONE#

Ideas:
- Loss combinations can be wrapped in a single criterion object  #DONE#
- Label smoothing 0.05 - 0.95 
    - no positive change for BCEWithLogitsLoss
    - Dice - shouldn't matter at all

What's wrong:
- Model implementation - try existing #DONE#
- Loss - tried multiple implementations
- Learning Rate - tried many
- Data - @@@
- Batch size - using 1
- Eval metrics - fixed label-smoothing issue

TODO to revert:
- shuffling val/test
- batch size 1
- loss DiceAndBCE - use it; don't store history
- dataset limit len
- loader['train'] = loader['test']
- LRScheduler, EarlyStopping patience

Dice:
- problematic with all-zero and all-one tiles
- IoU:

Sweep:
- Loss
    - BCE weighing (>1 increases recall)
    - Dice smoothing
    - Dice: sample x batch version
    - BCE+Dice weighing
    - MSELoss
- LR
- optimizer

Doing:
- check grads
    - rise high
    - 000inf
- monitor both losses during training
- why does a perfect prediction get nonzero loss - OK

Order:
1) train as is: BCE(1e-2)+Dice
- ok, low grads
2) batch_size = 8
3) batch dice loss
3) pos_weight = 1 / labels.mean()

- no optimizer step, yet val metrics differ accross epochs
    training is the same -> batch normalization during training


Co udělá můj model s větším snímkem?
- repeat tile NxN: will output tile?
Umí onnx arbitrary input size?
Kvantizace x onnx a jiné exporty

Future work:
- Augmentations
- Fixed seed
- W&B Artifacts
